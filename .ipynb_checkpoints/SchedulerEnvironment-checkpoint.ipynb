{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08db570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import MultiDiscrete, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fa6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9524a",
   "metadata": {},
   "source": [
    "# Creating a Custom Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an environment\n",
    "import random\n",
    "import numpy as np\n",
    "class SchedulerEnvironment(gym.Env):\n",
    "    def __init__(self, scenario_one = True, terminate_num = 200, mean_delay_one = 6, mean_delay_two = 4, margin_of_delay = 1):\n",
    "        self.scenario_one = scenario_one\n",
    "        self.mean_delay_one = mean_delay_one\n",
    "        self.mean_delay_two = mean_delay_two\n",
    "        self.margin_of_delay = margin_of_delay\n",
    "\n",
    "\n",
    "        self.queue_one_incoming_packet = 0.0\n",
    "        self.queue_two_incoming_packet = 0.0\n",
    "        self.queue_best_effort_incoming_packet = 0.0 \n",
    "\n",
    "        self.incoming_packets = dict(\n",
    "            {\n",
    "                0: self.queue_one_incoming_packet,\n",
    "                1: self.queue_two_incoming_packet,\n",
    "                2: self.queue_best_effort_incoming_packet\n",
    "\n",
    "            }\n",
    "            )\n",
    "\n",
    "\n",
    "        self.step_counter = 0 # i.e. timeslots\n",
    "        self.terminate_num = terminate_num\n",
    "\n",
    "        # maximum size of the queue\n",
    "        self.max_queue_size = 10\n",
    "        \n",
    "        # can adjust size of max delay in timeslots after initial observations\n",
    "        MAX_DELAY = 100\n",
    "\n",
    "        queue_sample = np.arange(self.max_queue_size)\n",
    "        for i in range(self.max_queue_size):\n",
    "            queue_sample[i] = MAX_DELAY\n",
    "\n",
    "        self.observation_space = spaces.Dict( \n",
    "            {\n",
    "                \n",
    "                \"queues\": MultiDiscrete(queue_sample), \n",
    "                                \n",
    "#                 boolean for timeslot delay during switch \n",
    "#                 zero for no delay, one for delay. \n",
    "                \"switchCounter\": Discrete(2)\n",
    "                \n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.queue_one_delay = [] \n",
    "\n",
    "        self.queue_two_delay = []\n",
    "\n",
    "        self.queue_best_effort_delay = [] \n",
    "\n",
    "        self.queue_delays = dict(\n",
    "            {\n",
    "                0: self.queue_one_delay,\n",
    "                1: self.queue_two_delay,\n",
    "                2: self.queue_best_effort_delay\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.queue_one_removed = 0\n",
    "        self.queue_two_removed = 0\n",
    "        self.queue_best_effort_removed = 0\n",
    "\n",
    "        self.removed_packets = dict(\n",
    "            {\n",
    "                0: self.queue_one_removed,\n",
    "                1: self.queue_two_removed,\n",
    "                2: self.queue_best_effort_removed\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.current_state = None\n",
    "        \n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        self._action_to_queue = {\n",
    "                0: self.queue_delays[0],\n",
    "                1: self.queue_delays[1],\n",
    "                2: self.queue_delays[2]\n",
    "            }\n",
    "\n",
    "    def _increment_delay(self):\n",
    "        for i in range(len(self.queue_delays)):    \n",
    "            for j in range(len(self.queue_delays[i])):\n",
    "                self.queue_delays[i][j] += 1\n",
    "\n",
    "    def _can_add_packets(self, packet_arrival, action ):\n",
    "        # increment before evaluating arrival. \n",
    "        self.incoming_packets[action] += packet_arrival\n",
    "        packet_arrived = self.incoming_packets[action] >= 1.0\n",
    "        not_max_size = len(self.queue_delays[action]) < self.max_queue_size\n",
    "\n",
    "        if packet_arrived and not_max_size:\n",
    "            self.incoming_packets[action] -= 1.0\n",
    "            self.queue_delays[action].append(0)                     \n",
    "\n",
    "    def _add_packets(self):   \n",
    "        PACKET_ARRIVAL_ONE = 0.3\n",
    "        PACKET_ARRIVAL_TWO = 0.25\n",
    "        PACKET_ARRIVAL_BEST_EFFORT = 0.4\n",
    "        \n",
    "        self._can_add_packets(PACKET_ARRIVAL_ONE, 0)\n",
    "        self._can_add_packets(PACKET_ARRIVAL_TWO, 1)\n",
    "        self._can_add_packets(PACKET_ARRIVAL_BEST_EFFORT, 2)\n",
    "\n",
    "        return\n",
    "     \n",
    "    def _modify_states(self):\n",
    "        # will not calculate delay for freshly added packets. \n",
    "        self._increment_delay()\n",
    "        self._add_packets()\n",
    "        return \n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return { \n",
    "            \"queues\": self.queue_delays\n",
    "            }\n",
    "    \n",
    "    def _calculate_avg_delay(self, queue):\n",
    "        queue_size = len(queue)\n",
    "        if (queue_size == 0):\n",
    "            return 0\n",
    "        \n",
    "        # sum\n",
    "        sum = 0\n",
    "        for i in range(queue_size):\n",
    "            sum += queue[i]\n",
    "        \n",
    "        # avg\n",
    "        avg = sum / queue_size\n",
    "        return avg\n",
    "\n",
    "    def _get_info(self):\n",
    "\n",
    "        avg_one = self._calculate_avg_delay(self.queue_delays[0])\n",
    "        avg_two = self._calculate_avg_delay(self.queue_delays[1])\n",
    "        avg_best = self._calculate_avg_delay(self.queue_delays[2])\n",
    "\n",
    "        # return average delays for each queue. \n",
    "        avg_mean_all_queues = [avg_one, avg_two, avg_best]\n",
    "        \n",
    "        # return the number of packets removed in each queue. \n",
    "        \n",
    "        packets_removed = [self.removed_packets[0], self.removed_packets[1], self.removed_packets[2]]\n",
    "\n",
    "        # return the queue num.\n",
    "        return [avg_mean_all_queues, packets_removed, self.step_counter + 1]\n",
    " \n",
    "    def _initialise_delays(self, queue):\n",
    "        # give a random delay between 0 and 5\n",
    "        size_of_queue = len(queue)\n",
    "        for i in range(size_of_queue):\n",
    "            queue[i] = random.randint(0, 5)\n",
    "\n",
    "    def _set_all_zero(self):\n",
    "        for i in range((3)):\n",
    "            self.queue_delays[i].clear()\n",
    "            self.removed_packets[i] = 0\n",
    "\n",
    "        self.step_counter = 0\n",
    "        self.current_state = None\n",
    "        self.queue_one_incoming_packet = 0.0\n",
    "        self.queue_two_incoming_packet = 0.0\n",
    "        self.queue_best_effort_incoming_packet = 0.0 \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # reset everything \n",
    "        self._set_all_zero()\n",
    "\n",
    "        # set the size of the queue. \n",
    "        self.queue_delays[0] = self.observation_space[\"queues\"].sample().tolist()\n",
    "        self.queue_delays[1] = self.observation_space[\"queues\"].sample().tolist()\n",
    "        self.queue_delays[2] = self.observation_space[\"queues\"].sample().tolist()\n",
    "\n",
    "        # set seed before randomising \n",
    "        random.seed(seed)\n",
    "\n",
    "        self._initialise_delays(self.queue_delays[0])\n",
    "        self._initialise_delays(self.queue_delays[1])\n",
    "        self._initialise_delays(self.queue_delays[2])\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def _reward_function(self, action):       \n",
    "        reward = 0\n",
    "\n",
    "        mean_delays = dict(\n",
    "            {\n",
    "                0: self.mean_delay_one,\n",
    "                1: self.mean_delay_two\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if (action == 2):\n",
    "        \n",
    "            # reward taking the best effort queue when mean delays are adjusted\n",
    "            # for priority queues.\n",
    "            avg_first_queue = self._calculate_avg_delay(self.queue_delays[0])\n",
    "            within_margin_first_queue = ( avg_first_queue <= mean_delays[0] + self.margin_of_delay)\n",
    "            \n",
    "            avg_second_queue = self._calculate_avg_delay(self.queue_delays[1])\n",
    "            within_margin_second_queue = ( avg_second_queue <= mean_delays[1] + self.margin_of_delay)\n",
    "            \n",
    "            if (within_margin_first_queue and within_margin_second_queue):\n",
    "                reward = 1\n",
    "\n",
    "            # Punish for not prioritising. \n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "        else:\n",
    "            avg_delay = self._calculate_avg_delay(self.queue_delays[0])\n",
    "            # packet delay > mean delay by the margin\n",
    "            if (mean_delays[action] + self.margin_of_delay < avg_delay):\n",
    "                # encourage minimising the delay for this queue.\n",
    "                reward = 0\n",
    "\n",
    "            # packet delay < mean delay by the margin\n",
    "            elif (mean_delays[action] - self.margin_of_delay > avg_delay):\n",
    "                # discourage minimising the delay too much.\n",
    "                reward = -1\n",
    "\n",
    "            # packet delay within margin, don't perform again. \n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "        return reward \n",
    "\n",
    "    def _remove_packet(self, action):\n",
    "        # retrieve the packet. \n",
    "        self.queue_delays[action].pop(0)\n",
    "        self.removed_packets[action] += 1\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "\n",
    "        if len(self.queue_delays[action]) == 0:\n",
    "            reward = -1\n",
    "\n",
    "        else:\n",
    "            reward = self._reward_function(action)\n",
    "\n",
    "            if self.scenario_one:\n",
    "                self._remove_packet(action)\n",
    "    \n",
    "            else: \n",
    "                if (action == self.current_state):\n",
    "                    self._remove_packet(action)\n",
    "\n",
    "        # perform queue switch given the conditions.\n",
    "        if not self.scenario_one:\n",
    "\n",
    "            if (action != self.current_state):\n",
    "                self.current_state = action\n",
    "\n",
    "                # should consider if switching is the correct approach\n",
    "                # long term. Switching twice in a row is a waste of time.\n",
    "                reward -= 1\n",
    "                \n",
    "        self._modify_states()\n",
    "\n",
    "        info = self._get_info()\n",
    "\n",
    "        terminated = False\n",
    "        if self.step_counter + 1 == self.terminate_num:\n",
    "            terminated = True\n",
    "        self.step_counter += 1\n",
    "\n",
    "        # observation made after modifying states. \n",
    "        observation = self._get_obs()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SchedulerEnvironment(terminate_num = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(info):\n",
    "    print(\"----------\")\n",
    "    print(\"current step =\", info[2])\n",
    "    print(\"-----\")\n",
    "    print(\"Average delay of first queue: {:.2f}\".format(info[0][0]))\n",
    "    print(\"Average delay of second queue: {:.2f}\".format(info[0][1]))\n",
    "    print(\"Average delay of best-effort queue: {:.2f}\".format(info[0][2]))\n",
    "    print(\"-----\")\n",
    "    print(\"Packets removed from first queue:\", info[1][0])\n",
    "    print(\"Packets removed from second queue:\", info[1][1])\n",
    "    print(\"Packets removed from best-effort queue:\", info[1][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc525f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_observation(observation):\n",
    "    print(\"----------\")\n",
    "    print(\"First queue:\", observation[\"queues\"][0])\n",
    "    print(\"Second queue:\", observation[\"queues\"][1])\n",
    "    print(\"Best effort queue:\", observation[\"queues\"][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74ff5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "print_observation(obs)\n",
    "print_info(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7745c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_scheduler(seed = None):\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "\n",
    "    # reset seeds the randomiser\n",
    "    env.reset(seed)\n",
    "    score = 0\n",
    "\n",
    "    while not terminated:  \n",
    "    #     random \n",
    "        action = random.randint(0, 2)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        print_observation(obs)\n",
    "        print_info(info)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179022b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_scheduler(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f983325",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_scheduler(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ec392",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_scheduler(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c891dd3",
   "metadata": {},
   "source": [
    "FIFO tries to minimise all queues, which means the average delays for each queue converges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDFScheduler():  \n",
    "    \n",
    "    def __init__(self, scheduler_environment):\n",
    "        self.edf_env = scheduler_environment\n",
    "        self.current_packet_removed_one = 0\n",
    "        self.current_packet_removed_two = 0\n",
    "        self.current_packet_removed_best = 0\n",
    "        \n",
    "#         random intialisation values picked with the same mean delays. \n",
    "        self.deadline_one = 6\n",
    "        self.deadline_two = 4\n",
    "        self.deadline_best_effort = 10\n",
    "        \n",
    "        self.deadlines = np.array([self.deadline_one, self.deadline_two, self.deadline_best_effort])\n",
    "    \n",
    "#     obs must be the observation returned by schedulerEnvironment. \n",
    "    def _edf_action(self, mean_delay_one, mean_delay_two, mean_delay_best_effort, info):\n",
    "        \n",
    "        expected_mean_delays = [mean_delay_one, mean_delay_two, mean_delay_best_effort]\n",
    "        \n",
    "#         random initial values. \n",
    "        mean_delay_one_increment = 6\n",
    "        mean_delay_two_increment = 4\n",
    "        mean_delay_best_effort_increment = 10\n",
    "        \n",
    "        increments = [mean_delay_one_increment, mean_delay_two_increment, mean_delay_best_effort_increment]\n",
    "\n",
    "#         ideally we only want to change the queue that was changed.\n",
    "#         how to obtain num? using get info we retrieve the previous packets + packets removed.\n",
    "        old_packet_removed_one = self.current_packet_removed_one\n",
    "        old_packet_removed_two = self.current_packet_removed_two\n",
    "        old_packet_removed_best = self.current_packet_removed_best\n",
    "        \n",
    "#         info[1][num] = number of removed packets for queue num. \n",
    "        self.current_packet_removed_one = info[1][0]\n",
    "        self.current_packet_removed_two = info[1][1]\n",
    "        self.current_packet_removed_best = info[1][2]\n",
    "        \n",
    "        difference = np.array([self.current_packet_removed_one - old_packet_removed_one])\n",
    "        difference = np.append(difference, self.current_packet_removed_two - old_packet_removed_two)\n",
    "        difference = np.append(difference, self.current_packet_removed_best - old_packet_removed_best)\n",
    "        \n",
    "#         the queue which the next deadline is to be decided\n",
    "        queue_deadline = np.argmax(difference, axis = 0)\n",
    "    \n",
    "#         increment deadlines given the conditions: mean delay < expected delay.  \n",
    "        if (info[0][queue_deadline] < expected_mean_delays[queue_deadline]):\n",
    "#         increment by the amount which the mean delay was off from the expected delay.\n",
    "            self.deadlines[queue_deadline] += increments[queue_deadline] \n",
    "            \n",
    "#         constant that modifies the level which the difference between mean delays is increased.\n",
    "#         this factor is multiplication as the values mean values are small for queue 1 and 2. \n",
    "            FACTOR_DEADLINE = 3\n",
    "            increment_deadline = (expected_mean_delays[queue_deadline] - info[0][queue_deadline]) * FACTOR_DEADLINE\n",
    "            self.deadlines[queue_deadline] += increment_deadline\n",
    "            \n",
    "        else:\n",
    "#             increment normally.\n",
    "            self.deadlines[queue_deadline] += increments[queue_deadline]\n",
    "            \n",
    "#     choose the smallest deadline to return\n",
    "        return np.argmin(self.deadlines, axis = 0)\n",
    "    \n",
    "#     mean_delay_one += mean_delay_one_increment\n",
    "    def perform_scheduling(self, mean_delay_one, mean_delay_two, mean_delay_best_effort, seed = None):\n",
    "        terminated = False \n",
    "        truncated = False\n",
    "        obs, info = self.edf_env.reset(seed)\n",
    "        score = 0\n",
    "        while not terminated: \n",
    "\n",
    "        #     edf\n",
    "            action = self._edf_action(mean_delay_one, mean_delay_two, mean_delay_best_effort, info)\n",
    "            obs, reward, terminated, truncated, info = self.edf_env.step(action)\n",
    "            score += reward\n",
    "            print_observation(obs)\n",
    "            print_info(info)\n",
    "        print(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2725b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefine the expected mean delay for first queue, second queue and best effort queue. \n",
    "mean_delay_one = 6\n",
    "mean_delay_two = 4\n",
    "mean_delay_best_effort = 10\n",
    "\n",
    "edf_env = SchedulerEnvironment(terminate_num = 50, mean_delay_one = mean_delay_one, mean_delay_two = mean_delay_two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daebdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_scheduler = EDFScheduler(edf_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e39db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edf_scheduler.perform_scheduling(mean_delay_one, mean_delay_two, mean_delay_best_effort, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_scheduler.perform_scheduling(mean_delay_one, mean_delay_two, mean_delay_best_effort, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_scheduler.perform_scheduling(mean_delay_one, mean_delay_two, mean_delay_best_effort, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdae75",
   "metadata": {},
   "source": [
    "We can see that the EDF is already performing much better than the random queue. \n",
    "We infer that the EDF scheduler attempts to keep the second queue very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritySchedulerWrapper(SchedulerEnvironment):\n",
    "    def __init__(self, scenario_one = True, terminate_num = 200, mean_delay_one = 6, mean_delay_two = 4, margin_of_delay = 1):\n",
    "        super().__init__(scenario_one = scenario_one,\n",
    "                       terminate_num = terminate_num,\n",
    "                       mean_delay_one = mean_delay_one,\n",
    "                       mean_delay_two = mean_delay_two,\n",
    "                       margin_of_delay = margin_of_delay)\n",
    "    \n",
    "    def prioritise_action(self):\n",
    "#         select the queue based on what reward can be gained.\n",
    "        action = 0\n",
    "        mean_delays = dict(\n",
    "            {\n",
    "                0: self.mean_delay_one,\n",
    "                1: self.mean_delay_two\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # choose the best action based on the current state. \n",
    "        avg_first_queue = self._calculate_avg_delay(self.queue_delays[0])\n",
    "        within_margin_first_queue = ( avg_first_queue <= mean_delays[0] + self.margin_of_delay)\n",
    "\n",
    "        avg_second_queue = self._calculate_avg_delay(self.queue_delays[1])\n",
    "        within_margin_second_queue = ( avg_second_queue <= mean_delays[1] + self.margin_of_delay)\n",
    "\n",
    "        if (within_margin_first_queue and within_margin_second_queue):\n",
    "            action = 2\n",
    "\n",
    "        else:\n",
    "#             do action = 1 if the first queue is within margin\n",
    "            if (within_margin_first_queue):\n",
    "                action = 1\n",
    "#                 action is otherwise 0.\n",
    "        return action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scheduler_env = PrioritySchedulerWrapper(terminate_num = 50, mean_delay_one = mean_delay_one, mean_delay_two = mean_delay_two )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_scheduler(priority_scheduler_env, seed = None):\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "\n",
    "    # reset seeds the randomiser\n",
    "    priority_scheduler_env.reset(seed)\n",
    "    score = 0\n",
    "\n",
    "    while not terminated:  \n",
    "    #     random \n",
    "        action = priority_scheduler_env.prioritise_action()\n",
    "        obs, reward, terminated, truncated, info = priority_scheduler_env.step(action)\n",
    "        score += reward\n",
    "        print_observation(obs)\n",
    "        print_info(info)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scheduler(priority_scheduler_env, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec554d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scheduler(priority_scheduler_env, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scheduler(priority_scheduler_env, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1fef5",
   "metadata": {},
   "source": [
    "This one has the best performance for scenario one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d115e",
   "metadata": {},
   "source": [
    "### QLearning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO:\n",
    "# Refactor class to handle different scheduler envs, actions, etc\n",
    "# Add Learning agent into QLearning function for learning\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_states, num_actions, learning=0.1, discount=0.99, epsilon=0.1):\n",
    "        self.QTable = np.zeros((num_states, num_actions))\n",
    "        self.learning = learning\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.QTable.shape[1])\n",
    "        else:\n",
    "            action = np.argmax(self.QTable[state])\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.discount * np.max(self.QTable[next_state])\n",
    "        td_error = td_target - self.QTable[state][action]\n",
    "        self.QTable[state][action] += self.learning * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, QTable, learning, discount, epsilon, episodes):\n",
    "    # Env: The OpenAI gym environment\n",
    "    # Q: Initial Q table\n",
    "    # learning: Learning Rate of Q learing\n",
    "    # discount: discount factor (gamma)\n",
    "    # epsilon: epsilon for exploration vs exploitation\n",
    "    # episodes: number of episodes to run when learing the Q table\n",
    "    \n",
    "    START_EPSILON_DECAYING = episodes // 10\n",
    "    END_EPSILON_DECAYING = episodes // 2\n",
    "    epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        total_reward, reward = 0,0\n",
    "        # get the initial state\n",
    "        state = env.reset()\n",
    "        observation, info = env.reset()\n",
    "        # TO DO: REVISE\n",
    "        discretState = discretize_state(state, bins, obsSpaceSize)\n",
    "        \n",
    "        steps = 0;\n",
    "        while done != True:   \n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy for explore vs exploitation\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                # select the best action according to Qtable (exploitation)\n",
    "                action = np.argmax(QTable[discretState])\n",
    "            else:\n",
    "                # select a random action (exploration)\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            # TO DO: Probably just change EDFScheduler's _edf_action to prioritise_action()\n",
    "            # Call a method on the scheduler object to get the best action\n",
    "            if isinstance(env, SchedulerEnvironment):\n",
    "                action = env.prioritise_action()\n",
    "            elif isinstance(env, PrioritySchedulerWrapper): \n",
    "                action = env.prioritise_action()\n",
    "            elif isinstance(env,EDFScheduler):\n",
    "                action = env._edf_action\n",
    "                \n",
    "            # Step and Get the next state and reward\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated \n",
    "            \n",
    "            discretStateNew = discretize_state(next_state, bins, obsSpaceSize)\n",
    "            \n",
    "            # TO DO: Don't think there is a terminal state\n",
    "                \n",
    "            # Update the Q table\n",
    "            QTable[discretState, action] = QTable[discretState, action] + \\\n",
    "                learning * (reward + discount * np.max(QTable[discretStateNew]) - QTable[discretState, action])\n",
    "                                     \n",
    "            # Update variables\n",
    "            total_reward += reward\n",
    "            discretState = discretStateNew ## REVISE\n",
    "            steps = steps + 1\n",
    "            \n",
    "        # Update epsilon\n",
    "        if END_EPSILON_DECAYING >= episode and episode >= START_EPSILON_DECAYING:\n",
    "            epsilon -= epsilon_decay_value\n",
    "        \n",
    "        # test the model and print test results\n",
    "        if episode % TEST_INTERVAL == 0:\n",
    "            success_run_ = list()\n",
    "            steps_ = list()\n",
    "            for i in range(N_TEST_RUNS):\n",
    "                success_run, steps = test_model(QTable)\n",
    "                success_run_.append(success_run)\n",
    "                steps_.append(steps)\n",
    "                \n",
    "            print('Testing at Episode {}:'.format(episode))\n",
    "            print('\\t Successful Runs: {}/{}'.format(np.sum(success_run_), N_TEST_RUNS) )\n",
    "            print('\\t Average Reward: {}'.format(np.mean(total_reward)))\n",
    "            print('\\t Average Steps: {}'.format(np.mean(steps_)))\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return QTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e34c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
