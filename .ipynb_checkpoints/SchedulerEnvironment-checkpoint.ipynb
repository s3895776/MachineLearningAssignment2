{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08db570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import MultiDiscrete, Discrete\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fa6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9524a",
   "metadata": {},
   "source": [
    "# Creating a Custom Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an environment\n",
    "import random\n",
    "import numpy as np\n",
    "class SchedulerEnvironment(gym.Env):\n",
    "    def __init__(self, scenario_one = True, terminate_num = 200, mean_delay_one = 6, mean_delay_two = 4, margin_of_delay = 1):\n",
    "        self.scenario_one = scenario_one\n",
    "        self.mean_delay_one = mean_delay_one\n",
    "        self.mean_delay_two = mean_delay_two\n",
    "        self.margin_of_delay = margin_of_delay\n",
    "\n",
    "\n",
    "        self.queue_one_incoming_packet = 0.0\n",
    "        self.queue_two_incoming_packet = 0.0\n",
    "        self.queue_best_effort_incoming_packet = 0.0 \n",
    "\n",
    "        self.incoming_packets = dict(\n",
    "            {\n",
    "                0: self.queue_one_incoming_packet,\n",
    "                1: self.queue_two_incoming_packet,\n",
    "                2: self.queue_best_effort_incoming_packet\n",
    "\n",
    "            }\n",
    "            )\n",
    "\n",
    "\n",
    "        self.step_counter = 0 # i.e. timeslots\n",
    "        self.terminate_num = terminate_num\n",
    "\n",
    "        # maximum size of the queue\n",
    "        self.max_queue_size = 10\n",
    "        \n",
    "        # can adjust size of max delay in timeslots after initial observations\n",
    "        self.MAX_DELAY = 100\n",
    "\n",
    "#         observation space is the average of the mean delays. \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"queues\": spaces.Box(low = 0, high = self.MAX_DELAY, shape = (3,self.max_queue_size), dtype=int),\n",
    "            \"avg_delays\": spaces.Box(low = 0, high = self.MAX_DELAY, shape = (1,3), dtype = int)\n",
    "        })\n",
    "\n",
    "        self.queue_one_delay = [] \n",
    "\n",
    "        self.queue_two_delay = []\n",
    "\n",
    "        self.queue_best_effort_delay = [] \n",
    "\n",
    "        self.queue_delays = dict(\n",
    "            {\n",
    "                0: self.queue_one_delay,\n",
    "                1: self.queue_two_delay,\n",
    "                2: self.queue_best_effort_delay\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.queue_one_removed = 0\n",
    "        self.queue_two_removed = 0\n",
    "        self.queue_best_effort_removed = 0\n",
    "\n",
    "        self.removed_packets = dict(\n",
    "            {\n",
    "                0: self.queue_one_removed,\n",
    "                1: self.queue_two_removed,\n",
    "                2: self.queue_best_effort_removed\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.current_state = None\n",
    "        \n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        self._action_to_queue = {\n",
    "                0: self.queue_delays[0],\n",
    "                1: self.queue_delays[1],\n",
    "                2: self.queue_delays[2]\n",
    "            }\n",
    "\n",
    "    def _increment_delay(self):\n",
    "        for i in range(len(self.queue_delays)):    \n",
    "            for j in range(len(self.queue_delays[i])):\n",
    "                if (self.queue_delays[i][j] < self.MAX_DELAY):\n",
    "                    self.queue_delays[i][j] += 1\n",
    "\n",
    "    def _can_add_packets(self, packet_arrival, action ):\n",
    "        # increment before evaluating arrival. \n",
    "        self.incoming_packets[action] += packet_arrival\n",
    "        packet_arrived = self.incoming_packets[action] >= 1.0\n",
    "        not_max_size = len(self.queue_delays[action]) < self.max_queue_size\n",
    "\n",
    "        if packet_arrived and not_max_size:\n",
    "            self.incoming_packets[action] -= 1.0\n",
    "            self.queue_delays[action].append(0)                     \n",
    "\n",
    "    def _add_packets(self):   \n",
    "        PACKET_ARRIVAL_ONE = 0.3\n",
    "        PACKET_ARRIVAL_TWO = 0.25\n",
    "        PACKET_ARRIVAL_BEST_EFFORT = 0.4\n",
    "        \n",
    "        self._can_add_packets(PACKET_ARRIVAL_ONE, 0)\n",
    "        self._can_add_packets(PACKET_ARRIVAL_TWO, 1)\n",
    "        self._can_add_packets(PACKET_ARRIVAL_BEST_EFFORT, 2)\n",
    "\n",
    "        return\n",
    "     \n",
    "    def _modify_states(self):\n",
    "        # will not calculate delay for freshly added packets. \n",
    "        self._increment_delay()\n",
    "        self._add_packets()\n",
    "        return \n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        avg_one = self._calculate_avg_delay(self.queue_delays[0])\n",
    "        avg_two = self._calculate_avg_delay(self.queue_delays[1])\n",
    "        avg_best = self._calculate_avg_delay(self.queue_delays[2])\n",
    "\n",
    "        # return average delays for each queue. \n",
    "        avg_mean_all_queues = [avg_one, avg_two, avg_best]\n",
    "        \n",
    "        return { \n",
    "            \"queues\": self.queue_delays,\n",
    "            \"avg_delays\": avg_mean_all_queues\n",
    "            }\n",
    "    \n",
    "    def _calculate_avg_delay(self, queue):\n",
    "        queue_size = len(queue)\n",
    "        if (queue_size == 0):\n",
    "            return 0\n",
    "        \n",
    "        # sum\n",
    "        sum = 0\n",
    "        for i in range(queue_size):\n",
    "            sum += queue[i]\n",
    "        \n",
    "        # avg\n",
    "        avg = sum / queue_size\n",
    "        return avg\n",
    "\n",
    "    def _get_info(self):\n",
    "\n",
    "        # return the number of packets removed in each queue. \n",
    "        \n",
    "        packets_removed = [self.removed_packets[0], self.removed_packets[1], self.removed_packets[2]]\n",
    "\n",
    "        # return the queue num.\n",
    "        return [packets_removed, self.step_counter + 1]\n",
    " \n",
    "    def _initialise_delays(self, queue):\n",
    "        # give a random delay between 0 and 5\n",
    "        size_of_queue = len(queue)\n",
    "        for i in range(size_of_queue):\n",
    "            queue[i] = random.randint(0, 5)\n",
    "\n",
    "    def _set_all_zero(self):\n",
    "        for i in range((3)):\n",
    "            self.queue_delays[i].clear()\n",
    "            self.removed_packets[i] = 0\n",
    "            self.incoming_packets[i] = 0.0\n",
    "\n",
    "        self.step_counter = 0\n",
    "        self.current_state = None\n",
    "        \n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        # reset everything \n",
    "        self._set_all_zero()\n",
    "\n",
    "        # set the size of the queue. \n",
    "        self.queue_delays[0] = self.observation_space[\"queues\"].sample().tolist()\n",
    "        self.queue_delays[1] = self.observation_space[\"queues\"].sample().tolist()\n",
    "        self.queue_delays[2] = self.observation_space[\"queues\"].sample().tolist()\n",
    "\n",
    "        # set seed before randomising \n",
    "        random.seed(seed)\n",
    "\n",
    "        self._initialise_delays(self.queue_delays[0])\n",
    "        self._initialise_delays(self.queue_delays[1])\n",
    "        self._initialise_delays(self.queue_delays[2])\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def _reward_function(self, action):       \n",
    "        reward = 0\n",
    "\n",
    "        mean_delays = dict(\n",
    "            {\n",
    "                0: self.mean_delay_one,\n",
    "                1: self.mean_delay_two\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if (action == 2):\n",
    "        \n",
    "            # reward taking the best effort queue when mean delays are adjusted\n",
    "            # for priority queues.\n",
    "            avg_first_queue = self._calculate_avg_delay(self.queue_delays[0])\n",
    "            within_margin_first_queue = ( avg_first_queue <= mean_delays[0] + self.margin_of_delay)\n",
    "            \n",
    "            avg_second_queue = self._calculate_avg_delay(self.queue_delays[1])\n",
    "            within_margin_second_queue = ( avg_second_queue <= mean_delays[1] + self.margin_of_delay)\n",
    "            \n",
    "            if (within_margin_first_queue and within_margin_second_queue):\n",
    "                reward = 1\n",
    "\n",
    "            # Punish for not prioritising. \n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "        else:\n",
    "            avg_delay = self._calculate_avg_delay(self.queue_delays[0])\n",
    "            # packet delay > mean delay by the margin\n",
    "            if (mean_delays[action] + self.margin_of_delay < avg_delay):\n",
    "                # encourage minimising the delay for this queue.\n",
    "                reward = 0\n",
    "\n",
    "            # packet delay < mean delay by the margin\n",
    "            elif (mean_delays[action] - self.margin_of_delay > avg_delay):\n",
    "                # discourage minimising the delay too much.\n",
    "                reward = -1\n",
    "\n",
    "            # packet delay within margin, don't perform again. \n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "        return reward \n",
    "\n",
    "    def _remove_packet(self, action):\n",
    "        # retrieve the packet. \n",
    "        self.queue_delays[action].pop(0)\n",
    "        self.removed_packets[action] += 1\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "\n",
    "        if len(self.queue_delays[action]) == 0:\n",
    "            reward = -1\n",
    "\n",
    "        else:\n",
    "            reward = self._reward_function(action)\n",
    "\n",
    "            if self.scenario_one:\n",
    "                self._remove_packet(action)\n",
    "    \n",
    "            else: \n",
    "                if (action == self.current_state):\n",
    "                    self._remove_packet(action)\n",
    "\n",
    "        # perform queue switch given the conditions.\n",
    "        if not self.scenario_one:\n",
    "\n",
    "            if (action != self.current_state):\n",
    "                self.current_state = action\n",
    "\n",
    "                # should consider if switching is the correct approach\n",
    "                # long term. Switching twice in a row is a waste of time.\n",
    "                reward -= 1\n",
    "                \n",
    "        self._modify_states()\n",
    "\n",
    "        info = self._get_info()\n",
    "\n",
    "        terminated = False\n",
    "        if self.step_counter + 1 == self.terminate_num:\n",
    "            terminated = True\n",
    "        self.step_counter += 1\n",
    "\n",
    "        # observation made after modifying states. \n",
    "        observation = self._get_obs()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "#     def close(self):\n",
    "#         if self.window is not None:\n",
    "#             pygame.display.quit()\n",
    "#             pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SchedulerEnvironment(terminate_num = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('State space Low: ', env.observation_space[\"queues\"].low)\n",
    "print('State space High: ', env.observation_space[\"queues\"].high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17024b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('State space Low: ', env.observation_space[\"avg_delays\"].low)\n",
    "print('State space High: ', env.observation_space[\"avg_delays\"].high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(info):\n",
    "    print(\"----------\")\n",
    "    print(\"current step =\", info[1])\n",
    "    print(\"-----\")\n",
    "    print(\"Packets removed from first queue:\", info[0][0])\n",
    "    print(\"Packets removed from second queue:\", info[0][1])\n",
    "    print(\"Packets removed from best-effort queue:\", info[0][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc525f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_observation(observation):\n",
    "    print(\"----------\")\n",
    "    print(\"First queue:\", observation[\"queues\"][0])\n",
    "    print(\"Second queue:\", observation[\"queues\"][1])\n",
    "    print(\"Best effort queue:\", observation[\"queues\"][2])\n",
    "    print(\"-----\")\n",
    "    print(\"Average delay of first queue: {:.2f}\".format(observation[\"avg_delays\"][0]))\n",
    "    print(\"Average delay of second queue: {:.2f}\".format(observation[\"avg_delays\"][1]))\n",
    "    print(\"Average delay of best-effort queue: {:.2f}\".format(observation[\"avg_delays\"][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74ff5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "print_observation(obs)\n",
    "print_info(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7745c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_scheduler(seed = None):\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "\n",
    "    # reset seeds the randomiser\n",
    "    env.reset(seed)\n",
    "    score = 0\n",
    "\n",
    "    while not terminated:  \n",
    "    #     random \n",
    "        action = random.randint(0, 2)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        print_observation(obs)\n",
    "        print_info(info)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179022b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_scheduler(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f983325",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_scheduler(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ec392",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_scheduler(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_scheduler_plot(seed = None, queue_observed = 0):\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "    \n",
    "    queue_observed = np.clip(queue_observed, 0 , 2)\n",
    "    \n",
    "#     mean delay of queues: observation[\"avg_delays\"][queue_observed]\n",
    "    y = []\n",
    "    \n",
    "#     step counter: info[1]\n",
    "    x = []\n",
    "    \n",
    "    # reset seeds the randomiser\n",
    "    env.reset(seed)\n",
    "    \n",
    "#     rand int needs its own seed. \n",
    "    random.seed(seed)\n",
    "    score = 0\n",
    "\n",
    "    while not terminated:  \n",
    "    #     random \n",
    "        action = random.randint(0, 2)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "#         score += reward\n",
    "\n",
    "#         print_observation(obs)\n",
    "        y.append(obs[\"avg_delays\"][queue_observed])\n",
    "#         print_info(info)\n",
    "        x.append(info[1])\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Mean delay of queue {}'.format(queue_observed + 1))\n",
    "    plt.ylabel('queue {}'.format(queue_observed + 1)) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_scheduler_plot(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70281d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_scheduler_hist(seed = None, queue_observed = 0):\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "    \n",
    "    queue_observed = np.clip(queue_observed, 0 , 2)\n",
    "    \n",
    "#     mean delay of queues: observation[\"avg_delays\"][queue_observed]\n",
    "    y = []\n",
    "    \n",
    "    # reset seeds the randomiser\n",
    "    env.reset(seed)\n",
    "    \n",
    "#     rand int needs its own seed. \n",
    "    random.seed(seed)\n",
    "    score = 0\n",
    "\n",
    "    while not terminated:  \n",
    "    #     random \n",
    "        action = random.randint(0, 2)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "#         score += reward\n",
    "#         print_observation(obs)\n",
    "        y.append(len(obs[\"queues\"][queue_observed]))\n",
    "\n",
    "    plt.hist(y)\n",
    "    plt.xlabel('Length of queue {}'.format(queue_observed + 1))\n",
    "    plt.ylabel('Frequency') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ced34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_scheduler_hist(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDFSchedulerWrapper(SchedulerEnvironment):  \n",
    "    \n",
    "    def __init__(self, scenario_one = True, terminate_num = 200, mean_delay_one = 6, mean_delay_two = 4, margin_of_delay = 1):\n",
    "        super().__init__(scenario_one = scenario_one,\n",
    "                       terminate_num = terminate_num,\n",
    "                       mean_delay_one = mean_delay_one,\n",
    "                       mean_delay_two = mean_delay_two,\n",
    "                       margin_of_delay = margin_of_delay)\n",
    "        \n",
    "        self.current_packet_removed_one = 0\n",
    "        self.current_packet_removed_two = 0\n",
    "        self.current_packet_removed_best = 0\n",
    "        \n",
    "#         random intialisation values picked with the same mean delays. \n",
    "        self.deadline_one = 6\n",
    "        self.deadline_two = 4\n",
    "        self.deadline_best_effort = 10\n",
    "        \n",
    "        self.deadlines = np.array([self.deadline_one, self.deadline_two, self.deadline_best_effort])\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.deadlines = np.array([self.deadline_one, self.deadline_two, self.deadline_best_effort])\n",
    "        return super().reset(seed)\n",
    "        \n",
    "    def _edf_action(self, queue_deadline):\n",
    "        \n",
    "#         predefine needed for EDF scheduler. \n",
    "        mean_delay_best_effort = 10\n",
    "        \n",
    "        expected_mean_delays = [self.mean_delay_one, self.mean_delay_two, mean_delay_best_effort]\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "#         random increment values. \n",
    "        mean_delay_one_increment = 6\n",
    "        mean_delay_two_increment = 4\n",
    "        mean_delay_best_effort_increment = 10\n",
    "        \n",
    "        increments = [mean_delay_one_increment, mean_delay_two_increment, mean_delay_best_effort_increment]\n",
    "    \n",
    "#         increment deadlines given the conditions: mean delay < expected delay.  \n",
    "        if (obs[\"avg_delays\"][queue_deadline] < expected_mean_delays[queue_deadline]):\n",
    "        \n",
    "#         increment by the amount which the mean delay was off from the expected delay.\n",
    "            self.deadlines[queue_deadline] += increments[queue_deadline] \n",
    "            \n",
    "#         constant that modifies the level which the difference between mean delays is increased.\n",
    "#         perform multiplication as the expected mean values are small for queue 1 and 2. \n",
    "            FACTOR_DEADLINE = 7\n",
    "            increment_deadline = (expected_mean_delays[queue_deadline] - obs[\"avg_delays\"][queue_deadline]) * FACTOR_DEADLINE\n",
    "            self.deadlines[queue_deadline] += increment_deadline\n",
    "            \n",
    "        else:\n",
    "#             increment normally.\n",
    "            self.deadlines[queue_deadline] += increments[queue_deadline]\n",
    "            \n",
    "#     choose the smallest deadline to return as the action. \n",
    "        return np.argmin(self.deadlines, axis = 0)\n",
    "    \n",
    "#     mean_delay_one += mean_delay_one_increment\n",
    "    def perform_scheduling(self, seed = None):\n",
    "        terminated = False \n",
    "        truncated = False\n",
    "        \n",
    "        obs, info = self.reset(seed)\n",
    "        \n",
    "#         #  0th step \n",
    "#         print_observation(obs)\n",
    "#         print_info(info)\n",
    "        \n",
    "#         print(\"Deadlines:\", self.deadlines)\n",
    "        score = 0\n",
    "        action = np.argmin(self.deadlines, axis = 0)\n",
    "        while not terminated: \n",
    "\n",
    "        #     edf\n",
    "            obs, reward, terminated, truncated, info = self.step(action)\n",
    "            action = self._edf_action(action)\n",
    "            score += reward\n",
    "            print_observation(obs)\n",
    "            print_info(info)\n",
    "        print(score)\n",
    "        \n",
    "    \n",
    "    def edf_scheduler_hist(self, seed = None, queue_observed = 0):        \n",
    "        terminated = False \n",
    "        truncated = False\n",
    "        obs, info = self.reset(seed)\n",
    "\n",
    "        queue_observed = np.clip(queue_observed, 0 , 2)\n",
    "\n",
    "    #     mean delay of queues: observation[\"avg_delays\"][queue_observed]\n",
    "        y = []\n",
    "        \n",
    "        action = np.argmin(self.deadlines, axis = 0)\n",
    "        while not terminated:  \n",
    "        #     edf \n",
    "            obs, reward, terminated, truncated, info = self.step(action)\n",
    "            action = self._edf_action(action)\n",
    "        \n",
    "            y.append(len(obs[\"queues\"][queue_observed]))\n",
    "\n",
    "        plt.hist(y)\n",
    "        plt.xlabel('Mean delay of queue {}'.format(queue_observed + 1))\n",
    "        plt.ylabel('Frequency') \n",
    "        plt.show()\n",
    "        \n",
    "    def edf_scheduler_plot(self, seed = None, queue_observed = 0):\n",
    "        terminated = False \n",
    "        truncated = False\n",
    "        obs, info = self.reset(seed)\n",
    "\n",
    "        queue_observed = np.clip(queue_observed, 0 , 2)\n",
    "\n",
    "    #     mean delay of queues: observation[\"avg_delays\"][queue_observed]\n",
    "        y = []\n",
    "\n",
    "    #     step counter: info[1]\n",
    "        x = []\n",
    "        \n",
    "        #  0th step \n",
    "        print_observation(obs)\n",
    "        print_info(info)\n",
    "        \n",
    "        print(\"Deadlines:\", self.deadlines)\n",
    "\n",
    "        action = np.argmin(self.deadlines, axis = 0)\n",
    "        while not terminated:  \n",
    "        #     edf \n",
    "            obs, reward, terminated, truncated, info = self.step(action)\n",
    "            action = self._edf_action(action)\n",
    "            \n",
    "            y.append(obs[\"avg_delays\"][queue_observed])\n",
    "            x.append(info[1])\n",
    "\n",
    "        plt.plot(x, y)\n",
    "        plt.xlabel('Mean delay of queue {}'.format(queue_observed + 1))\n",
    "        plt.ylabel('queue {}'.format(queue_observed + 1)) \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bec9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefine the expected mean delay for first queue, second queue and best effort queue. \n",
    "mean_delay_one = 6\n",
    "mean_delay_two = 4\n",
    "mean_delay_best_effort = 10\n",
    "\n",
    "edf_env = EDFSchedulerWrapper(terminate_num = 50, mean_delay_one = mean_delay_one, mean_delay_two = mean_delay_two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f386fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edf_env.perform_scheduling(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e40a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "edf_env.edf_scheduler_plot(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_env.edf_scheduler_hist(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdae75",
   "metadata": {},
   "source": [
    "We can see that the EDF is already performing much better than the random queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba0346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b50c32e0",
   "metadata": {},
   "source": [
    "We infer that the EDF scheduler attempts to keep the second queue very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritySchedulerWrapper(SchedulerEnvironment):\n",
    "    def __init__(self, scenario_one = True, terminate_num = 200, mean_delay_one = 6, mean_delay_two = 4, margin_of_delay = 1):\n",
    "        super().__init__(scenario_one = scenario_one,\n",
    "                       terminate_num = terminate_num,\n",
    "                       mean_delay_one = mean_delay_one,\n",
    "                       mean_delay_two = mean_delay_two,\n",
    "                       margin_of_delay = margin_of_delay)\n",
    "    \n",
    "    def prioritise_action(self):\n",
    "#         select the queue based on what reward can be gained.\n",
    "        action = 0\n",
    "        mean_delays = dict(\n",
    "            {\n",
    "                0: self.mean_delay_one,\n",
    "                1: self.mean_delay_two\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # choose the best action based on the current state. \n",
    "        avg_first_queue = self._calculate_avg_delay(self.queue_delays[0])\n",
    "        within_margin_first_queue = ( avg_first_queue <= mean_delays[0] + self.margin_of_delay)\n",
    "\n",
    "        avg_second_queue = self._calculate_avg_delay(self.queue_delays[1])\n",
    "        within_margin_second_queue = ( avg_second_queue <= mean_delays[1] + self.margin_of_delay)\n",
    "\n",
    "        if (within_margin_first_queue and within_margin_second_queue):\n",
    "            action = 2\n",
    "\n",
    "        else:\n",
    "#             do action = 1 if the first queue is within margin\n",
    "            if (within_margin_first_queue):\n",
    "                action = 1\n",
    "#                 action is otherwise 0.\n",
    "        return action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scheduler_env = PrioritySchedulerWrapper(terminate_num = 50, mean_delay_one = mean_delay_one, mean_delay_two = mean_delay_two )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_scheduler(priority_scheduler_env, seed = None):\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "\n",
    "    # reset seeds the randomiser\n",
    "    priority_scheduler_env.reset(seed)\n",
    "    score = 0\n",
    "\n",
    "    while not terminated:  \n",
    "    #     random \n",
    "        action = priority_scheduler_env.prioritise_action()\n",
    "        obs, reward, terminated, truncated, info = priority_scheduler_env.step(action)\n",
    "        score += reward\n",
    "        print_observation(obs)\n",
    "        print_info(info)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scheduler(priority_scheduler_env, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec554d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scheduler(priority_scheduler_env, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534adbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "priority_scheduler(priority_scheduler_env, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority_scheduler_hist(priority_scheduler_env, seed = None, queue_observed = 0):\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "    \n",
    "    queue_observed = np.clip(queue_observed, 0 , 2)\n",
    "    \n",
    "#     mean delay of queues: observation[\"avg_delays\"][queue_observed]\n",
    "    y = []\n",
    "    \n",
    "    # reset seeds the randomiser\n",
    "    priority_scheduler_env.reset(seed)\n",
    "\n",
    "    while not terminated:  \n",
    "    #     prioritise action\n",
    "        action = priority_scheduler_env.prioritise_action()\n",
    "        obs, reward, terminated, truncated, info = priority_scheduler_env.step(action)\n",
    "\n",
    "        y.append(len(obs[\"queues\"][queue_observed]))\n",
    "\n",
    "    plt.hist(y)\n",
    "    plt.xlabel('Length of queue {}'.format(queue_observed + 1))\n",
    "    plt.ylabel('Frequency') \n",
    "    plt.show()\n",
    "\n",
    "def priority_scheduler_plot(priority_scheduler_env, seed = None, queue_observed = 0):\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "    \n",
    "    queue_observed = np.clip(queue_observed, 0 , 2)\n",
    "    \n",
    "#     mean delay of queues: observation[\"avg_delays\"][queue_observed]\n",
    "    y = []\n",
    "    \n",
    "#     step counter: info[1]\n",
    "    x = []\n",
    "    \n",
    "    # reset seeds the randomiser\n",
    "    priority_scheduler_env.reset(seed)\n",
    "    score = 0\n",
    "\n",
    "    while not terminated:  \n",
    "    #     prioritise action\n",
    "        action = priority_scheduler_env.prioritise_action()\n",
    "        obs, reward, terminated, truncated, info = priority_scheduler_env.step(action)\n",
    "\n",
    "        y.append(obs[\"avg_delays\"][queue_observed])\n",
    "        x.append(info[1])\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Step counter' )\n",
    "    plt.ylabel('Mean delay of queue {}'.format(queue_observed + 1)) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada38cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scheduler_hist(priority_scheduler_env, seed = 0, queue_observed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a116b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "priority_scheduler_plot(priority_scheduler_env, seed = 1, queue_observed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1fef5",
   "metadata": {},
   "source": [
    "This one has the best performance for scenario one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d115e",
   "metadata": {},
   "source": [
    "### QLearning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb93fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "\n",
    "    if isinstance(state, tuple):\n",
    "        queues = state[0]['avg_delays']\n",
    "        q1 = state[0]['avg_delays'][0]\n",
    "        q2 = state[0]['avg_delays'][1]\n",
    "        q3 = state[0]['avg_delays'][2]\n",
    "    else:\n",
    "        queues = state['avg_delays']\n",
    "        q1 = state['avg_delays'][0]\n",
    "        q2 = state['avg_delays'][1]\n",
    "        q3 = state['avg_delays'][2]\n",
    "        \n",
    "#     for q in queues:\n",
    "#         while len(queues[q]) < 10:\n",
    "#             queues[q].append(0)\n",
    "\n",
    "    queue_lists = [int(q1), int(q2), int(q3)]  \n",
    "    arr = np.array(queue_lists)\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_index(state):\n",
    "    state_index = state[0] * 101 ** 2 + state[1] * 101 + state[2]\n",
    "    return state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, QTable, learning, discount, epsilon, episodes):\n",
    "    # Env: The OpenAI gym environment\n",
    "    # Q: Initial Q table\n",
    "    # learning: Learning Rate of Q learing\n",
    "    # discount: discount factor (gamma)\n",
    "    # epsilon: epsilon for exploration vs exploitation\n",
    "    # episodes: number of episodes to run when learing the Q table\n",
    "    \n",
    "    # Initialize variables to hold rewards\n",
    "    reward_list = []\n",
    "    \n",
    "    START_EPSILON_DECAYING = 1\n",
    "    END_EPSILON_DECAYING = episodes // 2\n",
    "    epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        total_reward, reward = 0,0\n",
    "        # get the initial state\n",
    "        state = env.reset()\n",
    "        observation, info = env.reset()\n",
    "        # TO DO: DONT NEED AS OUR SPACE IS ALREADY DISCRETE\n",
    "        discretState = discretize_state(state)\n",
    "#         print(discretState)\n",
    "        state_index = get_state_index(discretState)\n",
    "#         print(state_index)\n",
    "      \n",
    "        \n",
    "        steps = 0;\n",
    "        while done != True:   \n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy for explore vs exploitation\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                # select the best action according to Qtable (exploitation)\n",
    "                action = np.argmax(QTable[state_index]) % 3\n",
    "#                 print(action)\n",
    "            else:\n",
    "                # select a random action (exploration)\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "                \n",
    "            # Step and Get the next state and reward\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated \n",
    "            discretStateNew = discretize_state(next_state)\n",
    "\n",
    "            # TO DO: Don't think there is a terminal state\n",
    "                \n",
    "            # Update the Q table\n",
    "            QTable[discretState, action] = QTable[discretState, action] + \\\n",
    "                learning * (reward + discount * np.max(QTable[discretStateNew]) - QTable[discretState, action])\n",
    "#             QTable[state, action] = QTable[state, action] + \\\n",
    "#                 learning * (reward + discount * np.max(QTable[next_state[0], next_state[1]]) - QTable[state, action])\n",
    "\n",
    "                        \n",
    "            # Update variables\n",
    "            total_reward += reward\n",
    "            discretState = discretStateNew \n",
    "#             state = new_state\n",
    "            steps = steps + 1\n",
    "            \n",
    "        # Update epsilon\n",
    "        if END_EPSILON_DECAYING >= episode and episode >= START_EPSILON_DECAYING:\n",
    "            epsilon -= epsilon_decay_value\n",
    "            \n",
    "        # Track rewards\n",
    "        reward_list.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            reward_list = []\n",
    "            \n",
    "        if ( episode +1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(episode+1, ave_reward))\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return QTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e34c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = SchedulerEnvironment(terminate_num = 50) \n",
    "num_states = 101 ** 3\n",
    "\n",
    "print(num_states)\n",
    "Q = np.random.uniform(low = -1, high = 1, size = (num_states, 3, 3))\n",
    "# Run Q-learning algorithm\n",
    "num_steps = 10**3\n",
    "Q = QLearning(env, Q, 0.4, 0.8, 0.7, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade3905",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q)\n",
    "print(len(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b9200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_table_index(discretized_observation):\n",
    "#     binary if it was up to 100. \n",
    "    pos_0 = (discretized_observation[0]) * ( 101 ** 2) + (discretized_observation[1]) * 101 + (discretized_observation[2])\n",
    "    return pos_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing \n",
    "print(\"Expected: {}, got: {}\".format(101 * 101 * 101 - 1, get_q_table_index([100,100,100]) ))\n",
    "print(\"Expected: {}, got: {}\".format(4, get_q_table_index([0,0,4]) ))\n",
    "print(\"Expected: {}, got: {}\".format(0, get_q_table_index([0,0,0]) ))\n",
    "print(\"Expected: {}, got: {}\".format(101, get_q_table_index([0,1,0]) ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466d3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# y = 1234.5645\n",
    "# math.modf(y) # (0.xxxx, 1234.0)\n",
    "seed = None\n",
    "obs, info = env.reset(seed = seed)\n",
    "\n",
    "Q_learning_pos = discretize_state(obs)\n",
    "print(Q_learning_pos)\n",
    "\n",
    "# state_adj = get_q_table_index(discretize_state(obs))\n",
    "\n",
    "done = False\n",
    "step_index = 0\n",
    "score = 0\n",
    "# print(Q)\n",
    "# print(Q[0][2])\n",
    "print(Q[101*101*101 - 1])\n",
    "\n",
    "while done != True:\n",
    "        \n",
    "#     print(Q[state_adj[0]][state_adj[1]])\n",
    "\n",
    "#     get the next action\n",
    "    state_adj = get_q_table_index(discretize_state(obs))\n",
    "    action = np.argmax(Q[state_adj][4]) \n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated \n",
    "    \n",
    "    print_observation(obs)                      \n",
    "    print_info(info)\n",
    "    score += reward\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4a399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
